{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Core dependencies\n",
        "!pip install gradio\n",
        "!pip install python-dotenv\n",
        "!pip install numpy\n",
        "!pip install pinecone-client\n",
        "!pip install sentence-transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install youtube-transcript-api\n",
        "!pip install google-api-python-client\n",
        "!pip install requests\n",
        "!pip install PyAudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BPIkjkfkpUqU",
        "outputId": "009d8403-ddf3-423f-e44f-281d8f07e6ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.25.2)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Error loading HuggingFace model: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 28.12 MiB is free. Process 11859 has 14.71 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 148.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "YouTube API initialized successfully\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.1.31)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.164.0)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: PyAudio in /usr/local/lib/python3.11/dist-packages (0.2.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxy00apvqBuM",
        "outputId": "b4928068-4335-4f8c-ae42-a2f04551cff3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.1.31)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/video_embeddings')\n",
        "from video_embeddings import VideoEmbeddingManager\n"
      ],
      "metadata": {
        "id": "vURhEQgRvLEs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('/content/.env')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLQ6opwdwr1E",
        "outputId": "d4120ed3-2f53-440a-d38a-1b18bf0c052d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y portaudio19-dev\n",
        "\n",
        "!pip install pyaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o97PqrIgvuVj",
        "outputId": "a1aad6bd-6fef-4414-8016-494fb7b4d196"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "portaudio19-dev is already the newest version (19.6.0-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Requirement already satisfied: pyaudio in /usr/local/lib/python3.11/dist-packages (0.2.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAql4q4V-FZR",
        "outputId": "dfa013b0-77bf-4da7-f6c8-eb72c70c173d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AssemblyAI API Key loaded for requests.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d9ddeebcbd9bba2823.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "import pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from typing import List, Dict, Any, Optional\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from video_embeddings import VideoEmbeddingManager\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import pyaudio  # Add PyAudio import\n",
        "import wave     # Add wave import\n",
        "import tempfile  # Add tempfile import\n",
        "# Add transformers and PEFT libraries for the HuggingFace model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# AssemblyAI API configuration using requests - Using exact key and parameters from test.py\n",
        "API_KEY = \"f6b537a6b4f140dfbead28720751b78e\"\n",
        "HEADERS = {\n",
        "    \"authorization\": API_KEY,\n",
        "    \"content-type\": \"application/json\"\n",
        "}\n",
        "API_ENDPOINT = \"https://api.assemblyai.com/v2/transcript\"\n",
        "UPLOAD_ENDPOINT = \"https://api.assemblyai.com/v2/upload\"\n",
        "\n",
        "print(\"AssemblyAI API Key loaded for requests.\")\n",
        "\n",
        "# Custom CSS for dynamic background only\n",
        "custom_css = \"\"\"\n",
        "@keyframes gradient {\n",
        "    0% { background-position: 0% 50%; }\n",
        "    50% { background-position: 100% 50%; }\n",
        "    100% { background-position: 0% 50%; }\n",
        "}\n",
        "\n",
        ".gradio-container {\n",
        "    background: linear-gradient(-45deg, #000000, #156A70, #000000, #074044);\n",
        "    background-size: 400% 400%;\n",
        "    animation: gradient 15s ease infinite;\n",
        "}\n",
        "\n",
        "/* Remove boxes around sections and radio button groups */\n",
        ".gradio-radio,\n",
        ".gradio-group {\n",
        "    border: none !important;\n",
        "    box-shadow: none !important;\n",
        "}\n",
        "\n",
        "/* Change color of all input boxes */\n",
        ".gradio-textbox input,\n",
        ".gradio-textbox textarea,\n",
        ".gradio-dropdown,\n",
        ".gradio-radio label,\n",
        ".gradio-checkbox label,\n",
        ".gradio-slider,\n",
        ".gradio-checkbox,\n",
        ".gradio-button {\n",
        "    border-color: #1CA9B3 !important;\n",
        "}\n",
        "\n",
        ".gradio-textbox,\n",
        ".gradio-dropdown,\n",
        ".gradio-slider,\n",
        ".gradio-checkbox,\n",
        ".gradio-radio,\n",
        ".gradio-button {\n",
        "    border-color: #1CA9B3 !important;\n",
        "}\n",
        "\n",
        ".gradio-button {\n",
        "    background-color: #1CA9B3 !important;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "class BaseSystem:\n",
        "    def __init__(self, pinecone_api_key: str, index_name: str,\n",
        "                 model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        \"\"\"\n",
        "        Initialize base system with Pinecone and models.\n",
        "        \"\"\"\n",
        "        # Initialize Pinecone client\n",
        "        self.pc = pinecone.Pinecone(api_key=pinecone_api_key)\n",
        "        self.index = self.pc.Index(index_name)\n",
        "\n",
        "        # Initialize HuggingFace model and tokenizer\n",
        "        self.hf_model_id = \"rishikann/qa_finetuned_model\"\n",
        "\n",
        "        # Check for GPU availability\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(model_name).to(self.device)\n",
        "\n",
        "        # Set parameters\n",
        "        self.top_k = 1  # Retrieve only the top source\n",
        "\n",
        "        # Initialize VideoEmbeddingManager\n",
        "        self.video_embedding_manager = VideoEmbeddingManager(\n",
        "            pinecone_api_key=pinecone_api_key,\n",
        "            index_name=index_name,\n",
        "            model_name=model_name\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Initialize HuggingFace model and tokenizer\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.hf_model_id)\n",
        "\n",
        "            # Load the base model with the adapter (finetuned model)\n",
        "            self.hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.hf_model_id,\n",
        "                device_map=self.device,\n",
        "                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32\n",
        "            )\n",
        "            print(f\"Successfully loaded finetuned model: {self.hf_model_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading HuggingFace model: {e}\")\n",
        "            self.hf_model = None\n",
        "            self.tokenizer = None\n",
        "\n",
        "        # Initialize YouTube API client\n",
        "        youtube_api_key = os.getenv('YOUTUBE_API_KEY')\n",
        "        if youtube_api_key:\n",
        "            try:\n",
        "                self.youtube = build('youtube', 'v3', developerKey=youtube_api_key)\n",
        "                self.youtube_available = True\n",
        "                print(\"YouTube API initialized successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Failed to initialize YouTube API client. Error: {e}\")\n",
        "                self.youtube_available = False\n",
        "        else:\n",
        "            print(\"Warning: YOUTUBE_API_KEY not found in environment variables\")\n",
        "            self.youtube_available = False\n",
        "\n",
        "        # Initialize NLP tools\n",
        "        try:\n",
        "            nltk.download('stopwords', quiet=True)\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: NLTK resource download issue. Error: {e}\")\n",
        "            self.stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are'}\n",
        "            self.lemmatizer = None\n",
        "\n",
        "        # Load spaCy model\n",
        "        try:\n",
        "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except:\n",
        "            print(\"Warning: spaCy model not found. Using simple pipeline.\")\n",
        "            self.nlp = spacy.blank(\"en\")\n",
        "\n",
        "        self.system_prompt = \"\"\"You are an expert educational assistant specialized in providing clear, accurate, and engaging learning experiences. Your primary goals are:\n",
        "\n",
        "1. Educational Excellence:\n",
        "   - Provide accurate, well-researched information\n",
        "   - Break down complex concepts into understandable parts\n",
        "   - Use clear explanations and examples\n",
        "   - Encourage critical thinking and deeper understanding\n",
        "\n",
        "2. Learning Support:\n",
        "   - Adapt explanations to the user's level of understanding\n",
        "   - Provide relevant examples and analogies\n",
        "   - Suggest additional learning resources when appropriate\n",
        "   - Encourage active learning and engagement\n",
        "\n",
        "3. Content Quality:\n",
        "   - Ensure information is up-to-date and reliable\n",
        "   - Present information in a structured, logical manner\n",
        "   - Use appropriate educational methodologies\n",
        "   - Maintain academic integrity and proper citations\n",
        "\n",
        "4. Engagement:\n",
        "   - Be encouraging and supportive\n",
        "   - Ask questions to check understanding\n",
        "   - Provide constructive feedback\n",
        "   - Create a positive learning environment\n",
        "\n",
        "Remember to:\n",
        "- Always prioritize educational value and learning outcomes\n",
        "- Be patient and supportive\n",
        "- Encourage questions and deeper exploration\n",
        "- Provide clear, structured explanations\n",
        "- Use appropriate educational methodologies\n",
        "- Maintain academic integrity\n",
        "\"\"\"\n",
        "\n",
        "    def extract_video_id(self, url: str) -> Optional[str]:\n",
        "        \"\"\"Extract YouTube video ID from URL.\"\"\"\n",
        "        patterns = [\n",
        "            r'(?:youtube\\.com\\/watch\\?v=|youtu\\.be\\/)([^&\\n?]+)',\n",
        "            r'youtube\\.com\\/embed\\/([^&\\n?]+)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, url)\n",
        "            if match:\n",
        "                return match.group(1)\n",
        "        return None\n",
        "\n",
        "    def get_video_transcript(self, video_id: str) -> str:\n",
        "        \"\"\"Get transcript from YouTube video.\"\"\"\n",
        "        try:\n",
        "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "            return ' '.join([entry['text'] for entry in transcript])\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting transcript: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def embed_query(self, query: str) -> List[float]:\n",
        "        \"\"\"Generate embedding for the query.\"\"\"\n",
        "        return self.embedding_model.encode(query, convert_to_tensor=True).cpu().tolist()\n",
        "\n",
        "    def retrieve_relevant_chunks(self, query_embedding: List[float]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Retrieve relevant chunks from Pinecone.\"\"\"\n",
        "        query_response = self.index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=self.top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        return [{\n",
        "            'score': match.score,\n",
        "            'text': match.metadata.get('text_sample', 'No text available')\n",
        "        } for match in query_response['matches']]\n",
        "\n",
        "    def search_relevant_video(self, query: str) -> Optional[str]:\n",
        "        \"\"\"Search for a relevant YouTube video based on the query.\"\"\"\n",
        "        if not hasattr(self, 'youtube_available') or not self.youtube_available:\n",
        "            print(\"YouTube API is not available. Skipping video search.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Extract keywords from the query\n",
        "            doc = self.nlp(query.lower())\n",
        "            keywords = [token.text for token in doc if not token.is_stop and\n",
        "                       (token.pos_ in ['NOUN', 'VERB', 'ADJ'] or len(token.text) > 3)]\n",
        "\n",
        "            if not keywords:\n",
        "                keywords = [query]\n",
        "\n",
        "            search_query = ' '.join(keywords[:3])\n",
        "            print(f\"Searching YouTube for: {search_query}\")\n",
        "\n",
        "            search_response = self.youtube.search().list(\n",
        "                q=search_query,\n",
        "                part='id,snippet',\n",
        "                maxResults=5,\n",
        "                type='video',\n",
        "                videoEmbeddable='true',\n",
        "                videoDuration='medium',\n",
        "                order='relevance'\n",
        "            ).execute()\n",
        "\n",
        "            if not search_response.get('items'):\n",
        "                print(\"No videos found for the query.\")\n",
        "                return None\n",
        "\n",
        "            video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
        "\n",
        "            videos_stats = self.youtube.videos().list(\n",
        "                part='statistics,snippet',\n",
        "                id=','.join(video_ids)\n",
        "            ).execute()\n",
        "\n",
        "            if not videos_stats.get('items'):\n",
        "                print(\"Failed to retrieve video statistics.\")\n",
        "                videos_info = []\n",
        "                for item in search_response['items']:\n",
        "                    video_id = item['id']['videoId']\n",
        "                    title = item['snippet']['title']\n",
        "                    description = item['snippet']['description']\n",
        "                    videos_info.append({\n",
        "                        'id': video_id,\n",
        "                        'title': title,\n",
        "                        'description': description\n",
        "                    })\n",
        "            else:\n",
        "                videos_info = []\n",
        "                for item in videos_stats['items']:\n",
        "                    video_id = item['id']\n",
        "                    title = item['snippet']['title']\n",
        "                    description = item['snippet']['description']\n",
        "                    view_count = int(item['statistics'].get('viewCount', 0))\n",
        "                    videos_info.append({\n",
        "                        'id': video_id,\n",
        "                        'title': title,\n",
        "                        'description': description,\n",
        "                        'view_count': view_count\n",
        "                    })\n",
        "\n",
        "                videos_info.sort(key=lambda x: x['view_count'], reverse=True)\n",
        "\n",
        "            if not videos_info:\n",
        "                print(\"No valid videos found.\")\n",
        "                return None\n",
        "\n",
        "            # Simple selection strategy: just take the first video with highest view count\n",
        "            # This replaces the LLM-based selection we were doing with Groq\n",
        "            for video in videos_info:\n",
        "                try:\n",
        "                    YouTubeTranscriptApi.get_transcript(video['id'])\n",
        "                    return f\"https://www.youtube.com/watch?v={video['id']}\"\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            # If we couldn't find a video with transcript, return the first one\n",
        "            if videos_info:\n",
        "                return f\"https://www.youtube.com/watch?v={videos_info[0]['id']}\"\n",
        "            return None\n",
        "\n",
        "        except HttpError as e:\n",
        "            print(f\"YouTube API HTTP error: {e.resp.status} {e.content}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while searching for videos: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "class EnhancedRAGSystem(BaseSystem):\n",
        "    def generate_answer(self, query: str, relevant_chunks: List[Dict[str, Any]], video_context: str = \"\", prompting_technique: str = \"standard\") -> str:\n",
        "        \"\"\"Generate a comprehensive answer using the finetuned model.\"\"\"\n",
        "        if not relevant_chunks and not video_context:\n",
        "            return \"I couldn't find sufficient information to answer your question.\"\n",
        "\n",
        "        if not self.hf_model or not self.tokenizer:\n",
        "            return \"The finetuned model could not be loaded. Please check the model configuration.\"\n",
        "\n",
        "        context = \"\"\n",
        "        if relevant_chunks:\n",
        "            context += f\"Knowledge Base Context:\\n{relevant_chunks[0]['text']}\\n\\n\"\n",
        "        if video_context:\n",
        "            context += f\"Video Context:\\n{video_context}\\n\\n\"\n",
        "\n",
        "        if prompting_technique.lower() == \"cot\":\n",
        "            prompt = self._generate_cot_prompt(query, context)\n",
        "        elif prompting_technique.lower() == \"tot\":\n",
        "            prompt = self._generate_tot_prompt(query, context)\n",
        "        elif prompting_technique.lower() == \"got\":\n",
        "            prompt = self._generate_got_prompt(query, context)\n",
        "        else:\n",
        "            prompt = self._generate_standard_prompt(query, context)\n",
        "\n",
        "        # Format the input for the model\n",
        "        model_input = f\"{self.system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = self.tokenizer(model_input, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generate the answer\n",
        "        with torch.no_grad():\n",
        "            outputs = self.hf_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # Decode the answer\n",
        "        answer = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _generate_standard_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an intelligent assistant specialized in educational content. Answer the question using the provided context.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Please provide a clear, concise answer based on the context. If the context doesn't contain enough information, say so.\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_cot_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an intelligent assistant using Chain of Thought reasoning. Let's solve this step by step.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's think through this:\n",
        "        1) First, let's understand what we know from the context\n",
        "        2) Then, let's break down the question\n",
        "        3) Finally, let's combine this information to form a complete answer\n",
        "\n",
        "        ANSWER:\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_tot_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an intelligent assistant using Tree of Thought reasoning. Let's explore different approaches.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's consider multiple perspectives:\n",
        "        Branch 1: Direct approach\n",
        "        Branch 2: Alternative interpretation\n",
        "        Branch 3: Combined approach\n",
        "\n",
        "        After evaluating all branches, here's the most comprehensive answer:\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_got_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an intelligent assistant using Graph of Thought reasoning. Let's analyze through interconnected concepts.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's map out the relationships:\n",
        "        1) Core concepts from the context\n",
        "        2) Related ideas and connections\n",
        "        3) Synthesis of all connected information\n",
        "\n",
        "        Based on this interconnected analysis, here's the answer:\n",
        "        \"\"\"\n",
        "\n",
        "class AgentSystem(BaseSystem):\n",
        "    def generate_answer(self, query: str, relevant_chunks: List[Dict[str, Any]], video_context: str = \"\", prompting_technique: str = \"standard\") -> str:\n",
        "        \"\"\"Generate a comprehensive answer using the finetuned model with agent capabilities.\"\"\"\n",
        "        if not relevant_chunks and not video_context:\n",
        "            return \"I couldn't find sufficient information to answer your question.\"\n",
        "\n",
        "        if not self.hf_model or not self.tokenizer:\n",
        "            return \"The finetuned model could not be loaded. Please check the model configuration.\"\n",
        "\n",
        "        context = \"\"\n",
        "        if relevant_chunks:\n",
        "            context += f\"Knowledge Base Context:\\n{relevant_chunks[0]['text']}\\n\\n\"\n",
        "        if video_context:\n",
        "            context += f\"Video Context:\\n{video_context}\\n\\n\"\n",
        "\n",
        "        if prompting_technique.lower() == \"cot\":\n",
        "            prompt = self._generate_cot_prompt(query, context)\n",
        "        elif prompting_technique.lower() == \"tot\":\n",
        "            prompt = self._generate_tot_prompt(query, context)\n",
        "        elif prompting_technique.lower() == \"got\":\n",
        "            prompt = self._generate_got_prompt(query, context)\n",
        "        else:\n",
        "            prompt = self._generate_standard_prompt(query, context)\n",
        "\n",
        "        # Format the input for the model\n",
        "        model_input = f\"{self.system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = self.tokenizer(model_input, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Generate the answer\n",
        "        with torch.no_grad():\n",
        "            outputs = self.hf_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.3,\n",
        "                top_p=0.9,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        # Decode the answer\n",
        "        answer = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    def _generate_standard_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an advanced AI agent capable of complex reasoning and problem-solving. Analyze the question and context carefully.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Please provide a detailed, well-reasoned answer that:\n",
        "        1. Analyzes the question thoroughly\n",
        "        2. Uses the context effectively\n",
        "        3. Provides additional insights\n",
        "        4. Considers multiple perspectives\n",
        "        5. Explains the reasoning process\n",
        "        6. Identifies information gaps\n",
        "        7. Suggests follow-up areas\n",
        "\n",
        "        Your answer should be structured, clear, and demonstrate deep understanding.\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_cot_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an advanced AI agent using Chain of Thought reasoning. Break down the problem and think step by step.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's analyze systematically:\n",
        "        1. Question Analysis:\n",
        "           - Core components\n",
        "           - Key requirements\n",
        "           - Hidden assumptions\n",
        "\n",
        "        2. Context Evaluation:\n",
        "           - Relevant information\n",
        "           - Missing elements\n",
        "           - Potential implications\n",
        "\n",
        "        3. Solution Development:\n",
        "           - Initial approach\n",
        "           - Alternative perspectives\n",
        "           - Synthesis and integration\n",
        "\n",
        "        4. Verification and Refinement:\n",
        "           - Logic check\n",
        "           - Completeness assessment\n",
        "           - Clarity enhancement\n",
        "\n",
        "        Based on this analysis, here's the comprehensive answer:\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_tot_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an advanced AI agent using Tree of Thought reasoning. Explore multiple solution paths.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's explore different solution paths:\n",
        "\n",
        "        Path 1: Analytical Approach\n",
        "        - Initial assumptions\n",
        "        - Logical steps\n",
        "        - Conclusions\n",
        "        - Limitations\n",
        "\n",
        "        Path 2: Contextual Approach\n",
        "        - Background considerations\n",
        "        - Contextual implications\n",
        "        - Practical applications\n",
        "        - Trade-offs\n",
        "\n",
        "        Path 3: Integrative Approach\n",
        "        - Cross-domain connections\n",
        "        - Synthesis opportunities\n",
        "        - Novel insights\n",
        "        - Future implications\n",
        "\n",
        "        After evaluating all paths, here's the optimal solution:\n",
        "        \"\"\"\n",
        "\n",
        "    def _generate_got_prompt(self, query: str, context: str) -> str:\n",
        "        return f\"\"\"\n",
        "        You are an advanced AI agent using Graph of Thought reasoning. Map and connect concepts.\n",
        "\n",
        "        USER QUESTION:\n",
        "        {query}\n",
        "\n",
        "        CONTEXT:\n",
        "        {context}\n",
        "\n",
        "        Let's create a conceptual network:\n",
        "\n",
        "        1. Core Concepts:\n",
        "           - Primary elements\n",
        "           - Key relationships\n",
        "           - Critical dependencies\n",
        "\n",
        "        2. Contextual Framework:\n",
        "           - Environmental factors\n",
        "           - System dynamics\n",
        "           - Emerging patterns\n",
        "\n",
        "        3. Integration Analysis:\n",
        "           - Cross-concept impacts\n",
        "           - Feedback loops\n",
        "           - Emergent properties\n",
        "\n",
        "        4. Solution Synthesis:\n",
        "           - Pattern recognition\n",
        "           - Insight generation\n",
        "           - Practical implications\n",
        "\n",
        "        Based on this network analysis, here's the comprehensive solution:\n",
        "        \"\"\"\n",
        "\n",
        "def create_system(mode: str):\n",
        "    \"\"\"Create and return the appropriate system based on mode.\"\"\"\n",
        "    if mode.lower() == \"rag\":\n",
        "        return EnhancedRAGSystem(\n",
        "            pinecone_api_key=os.getenv('PINECONE_API_KEY'),\n",
        "            index_name=os.getenv('PINECONE_INDEX_NAME', 'embeddings')\n",
        "        )\n",
        "    else:\n",
        "        return AgentSystem(\n",
        "            pinecone_api_key=os.getenv('PINECONE_API_KEY'),\n",
        "            index_name=os.getenv('PINECONE_INDEX_NAME', 'embeddings')\n",
        "        )\n",
        "\n",
        "def process_query(question: str, video_url: str = \"\", mode: str = \"RAG\", prompting_technique: str = \"standard\") -> tuple[str, str, str]:\n",
        "    \"\"\"Process the query and return answer, sources, and fetched URL.\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\", \"No sources available.\", \"\"\n",
        "\n",
        "    try:\n",
        "        system = create_system(mode)\n",
        "\n",
        "        # Generate embedding\n",
        "        query_embedding = system.embed_query(question)\n",
        "\n",
        "        # Get relevant chunks from knowledge base\n",
        "        relevant_chunks = system.retrieve_relevant_chunks(query_embedding)\n",
        "\n",
        "        # Check if knowledge should be expanded with the video\n",
        "        relevance_scores = [chunk['score'] for chunk in relevant_chunks]\n",
        "        should_expand = False\n",
        "\n",
        "        # Calculate average relevance score\n",
        "        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0\n",
        "        low_relevance = avg_relevance < 0.55\n",
        "\n",
        "        # Process video transcript\n",
        "        video_context = \"\"\n",
        "        if video_url:\n",
        "            video_id = system.extract_video_id(video_url)\n",
        "            if video_id:\n",
        "                try:\n",
        "                    video_context = system.get_video_transcript(video_id)\n",
        "                    if not video_context:\n",
        "                        print(f\"No transcript available for video: {video_id}\")\n",
        "                    else:\n",
        "                        should_expand = system.video_embedding_manager.should_expand_knowledge(relevance_scores)\n",
        "                        if should_expand:\n",
        "                            print(f\"Expanding knowledge with video as relevance score of existing knowledge base is low: {video_id}\")\n",
        "                            system.video_embedding_manager.process_video(video_url)\n",
        "                            relevant_chunks = system.retrieve_relevant_chunks(query_embedding)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error getting transcript: {str(e)}\")\n",
        "        elif low_relevance and hasattr(system, 'youtube_available') and system.youtube_available:\n",
        "            print(f\"Knowledge base relevance is low ({avg_relevance}). Searching for video...\")\n",
        "            relevant_video_url = system.search_relevant_video(question)\n",
        "            if relevant_video_url:\n",
        "                print(f\"Found relevant video: {relevant_video_url}\")\n",
        "                video_id = system.extract_video_id(relevant_video_url)\n",
        "                if video_id:\n",
        "                    try:\n",
        "                        video_context = system.get_video_transcript(video_id)\n",
        "                        if not video_context:\n",
        "                            print(f\"No transcript available for video: {video_id}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error getting transcript: {str(e)}\")\n",
        "                    video_url = relevant_video_url\n",
        "            else:\n",
        "                print(\"No relevant video found.\")\n",
        "\n",
        "        # Generate answer\n",
        "        answer = system.generate_answer(question, relevant_chunks, video_context, prompting_technique)\n",
        "\n",
        "        # Format sources\n",
        "        sources_text = \"\"\n",
        "        if relevant_chunks:\n",
        "            sources_text += \"Knowledge Base Sources:\\n\"\n",
        "            sources_text += \"\\n\\n\".join([\n",
        "                f\"Source (Relevance: {source['score']:.2f}):\\n{source['text']}\"\n",
        "                for source in relevant_chunks\n",
        "            ])\n",
        "\n",
        "        if video_context:\n",
        "            if sources_text:\n",
        "                sources_text += \"\\n\\n\"\n",
        "            sources_text += \"Video Context:\\n\" + video_context\n",
        "\n",
        "            if should_expand:\n",
        "                sources_text += \"\\n\\n[Knowledge base was expanded with this video as relevance score of existing knowledge base is low]\"\n",
        "\n",
        "        return answer, sources_text, video_url\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", \"Error retrieving sources.\", \"\"\n",
        "\n",
        "# Function to record audio using PyAudio (flexible duration)\n",
        "def record_audio_pyaudio():\n",
        "    \"\"\"Record audio from microphone with user-controlled start/stop using PyAudio\"\"\"\n",
        "    chunk = 1024\n",
        "    sample_format = pyaudio.paInt16\n",
        "    channels = 1\n",
        "    fs = 44100\n",
        "\n",
        "    # Create a temporary file to store the recording\n",
        "    temp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    filename = temp_file.name\n",
        "    temp_file.close()\n",
        "\n",
        "    # Global variable to store the current state\n",
        "    global recording_state\n",
        "    recording_state = {\"is_recording\": False, \"frames\": [], \"stream\": None, \"p\": None}\n",
        "\n",
        "    return \"Ready to start recording\"\n",
        "\n",
        "# Start recording function\n",
        "def start_recording():\n",
        "    \"\"\"Start the recording process\"\"\"\n",
        "    global recording_state\n",
        "\n",
        "    if recording_state[\"is_recording\"]:\n",
        "        return \"Already recording...\"\n",
        "\n",
        "    chunk = 1024\n",
        "    sample_format = pyaudio.paInt16\n",
        "    channels = 1\n",
        "    fs = 44100\n",
        "\n",
        "    recording_state[\"frames\"] = []\n",
        "    recording_state[\"p\"] = pyaudio.PyAudio()\n",
        "    recording_state[\"stream\"] = recording_state[\"p\"].open(\n",
        "        format=sample_format,\n",
        "        channels=channels,\n",
        "        rate=fs,\n",
        "        frames_per_buffer=chunk,\n",
        "        input=True\n",
        "    )\n",
        "    recording_state[\"is_recording\"] = True\n",
        "\n",
        "    # Start recording in a background thread\n",
        "    def record_thread():\n",
        "        while recording_state[\"is_recording\"]:\n",
        "            data = recording_state[\"stream\"].read(chunk)\n",
        "            recording_state[\"frames\"].append(data)\n",
        "\n",
        "    import threading\n",
        "    threading.Thread(target=record_thread, daemon=True).start()\n",
        "\n",
        "    return \"Recording in progress... Press 'Stop Recording' when finished.\"\n",
        "\n",
        "# Stop recording function\n",
        "def stop_recording():\n",
        "    \"\"\"Stop the recording process and save the audio file\"\"\"\n",
        "    global recording_state\n",
        "\n",
        "    if not recording_state[\"is_recording\"]:\n",
        "        return \"Not currently recording.\", \"\"\n",
        "\n",
        "    recording_state[\"is_recording\"] = False\n",
        "\n",
        "    if recording_state[\"stream\"]:\n",
        "        recording_state[\"stream\"].stop_stream()\n",
        "        recording_state[\"stream\"].close()\n",
        "\n",
        "    if recording_state[\"p\"]:\n",
        "        recording_state[\"p\"].terminate()\n",
        "\n",
        "    # Create a temporary file\n",
        "    temp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    filename = temp_file.name\n",
        "    temp_file.close()\n",
        "\n",
        "    if recording_state[\"frames\"]:\n",
        "        wf = wave.open(filename, 'wb')\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(recording_state[\"p\"].get_sample_size(pyaudio.paInt16))\n",
        "        wf.setframerate(44100)\n",
        "        wf.writeframes(b''.join(recording_state[\"frames\"]))\n",
        "        wf.close()\n",
        "\n",
        "        print(f\"Audio saved to: {filename}\")\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcription = transcribe_audio_assemblyai(filename)\n",
        "\n",
        "        # Clean up the temporary file\n",
        "        try:\n",
        "            os.remove(filename)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return \"Recording complete. Transcription done.\", transcription\n",
        "    else:\n",
        "        return \"No audio was recorded. Please try again.\", \"\"\n",
        "\n",
        "# Toggle recording function for single button\n",
        "def toggle_recording(button_text):\n",
        "    \"\"\"Toggle between recording start and stop\"\"\"\n",
        "    if button_text == \" Start Recording\":\n",
        "        # Start recording\n",
        "        result_status = start_recording()\n",
        "        return \" Stop Recording\", result_status, \"\"\n",
        "    else:\n",
        "        # Stop recording\n",
        "        result_status, transcription = stop_recording()\n",
        "        return \" Start Recording\", result_status, transcription\n",
        "\n",
        "# Function to transcribe audio using AssemblyAI with code directly from test.py\n",
        "def transcribe_audio_assemblyai(audio_file):\n",
        "    \"\"\"Send audio file to AssemblyAI for transcription, using the exact same code from test.py\"\"\"\n",
        "    print(\"Uploading audio file...\")\n",
        "\n",
        "    # Upload the audio file to AssemblyAI\n",
        "    with open(audio_file, \"rb\") as f:\n",
        "        response = requests.post(UPLOAD_ENDPOINT, headers=HEADERS, data=f)\n",
        "\n",
        "    audio_url = response.json()[\"upload_url\"]\n",
        "    print(f\"Audio file uploaded: {audio_url}\")\n",
        "\n",
        "    # Request transcription\n",
        "    transcript_request = {\n",
        "        \"audio_url\": audio_url,\n",
        "        \"language_code\": \"en\"  # Change if needed\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_ENDPOINT, json=transcript_request, headers=HEADERS)\n",
        "    transcript_id = response.json()[\"id\"]\n",
        "    print(f\"Transcription job submitted with ID: {transcript_id}\")\n",
        "\n",
        "    # Poll for transcription completion\n",
        "    polling_endpoint = f\"{API_ENDPOINT}/{transcript_id}\"\n",
        "\n",
        "    print(\"Waiting for transcription to complete...\")\n",
        "    while True:\n",
        "        response = requests.get(polling_endpoint, headers=HEADERS)\n",
        "        status = response.json()[\"status\"]\n",
        "\n",
        "        if status == \"completed\":\n",
        "            text = response.json()[\"text\"]\n",
        "            print(f\"Transcription completed: '{text}'\")\n",
        "            return text\n",
        "        elif status == \"error\":\n",
        "            print(\"Transcription error occurred.\")\n",
        "            return \"Transcription error occurred.\"\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=custom_css) as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    #  Enhanced RAG & Agent Q&A System\n",
        "    Choose between RAG or Agent mode and get answers based on the knowledge base and YouTube videos.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            mode_radio = gr.Radio(\n",
        "                choices=[\"RAG\", \"Agent\"],\n",
        "                value=\"RAG\",\n",
        "                label=\"Select Mode\",\n",
        "                info=\"RAG: Standard retrieval-augmented generation. Agent: Advanced reasoning with deeper analysis.\"\n",
        "            )\n",
        "            prompting_technique = gr.Radio(\n",
        "                choices=[\"standard\", \"cot\", \"tot\", \"got\"],\n",
        "                value=\"standard\",\n",
        "                label=\"Prompting Technique\",\n",
        "                info=\"standard: Basic reasoning, cot: Chain of Thought, tot: Tree of Thought, got: Graph of Thought\"\n",
        "            )\n",
        "\n",
        "            # Voice Input Section\n",
        "            gr.Markdown(\"\"\"\n",
        "            ###  Voice Input\n",
        "            Click 'Start Recording' to begin, then 'Stop Recording' when finished speaking.\n",
        "            Speak clearly into your microphone at a normal volume.\n",
        "            \"\"\")\n",
        "\n",
        "            # Create a toggle button for start and stop\n",
        "            toggle_btn = gr.Button(\" Start Recording\", variant=\"primary\")\n",
        "\n",
        "            recording_status = gr.Textbox(\n",
        "                label=\"Recording Status\",\n",
        "                value=\"Ready to record\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"Type your question here or use voice recording above...\",\n",
        "                lines=2\n",
        "            )\n",
        "            video_url_input = gr.Textbox(\n",
        "                label=\"YouTube Video URL (Optional)\",\n",
        "                placeholder=\"Paste a YouTube video URL here...\",\n",
        "                lines=1\n",
        "            )\n",
        "            fetched_url_output = gr.Textbox(\n",
        "                label=\"Automatically Fetched Video URL\",\n",
        "                placeholder=\"No video URL fetched yet...\",\n",
        "                lines=1,\n",
        "                interactive=False\n",
        "            )\n",
        "            submit_btn = gr.Button(\" Get Answer\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            answer_output = gr.Textbox(\n",
        "                label=\"Answer\",\n",
        "                lines=5,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "        with gr.Column(scale=2):\n",
        "            sources_output = gr.Textbox(\n",
        "                label=\"Sources\",\n",
        "                lines=5,\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    # Initialize recording state\n",
        "    recording_state = {\"is_recording\": False, \"frames\": [], \"stream\": None, \"p\": None}\n",
        "\n",
        "    # Set up the event handlers\n",
        "    toggle_btn.click(\n",
        "        fn=toggle_recording,\n",
        "        inputs=toggle_btn,\n",
        "        outputs=[toggle_btn, recording_status, question_input]\n",
        "    )\n",
        "\n",
        "    # Handle submission\n",
        "    submit_btn.click(\n",
        "        fn=process_query,\n",
        "        inputs=[question_input, video_url_input, mode_radio, prompting_technique],\n",
        "        outputs=[answer_output, sources_output, fetched_url_output]\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### Tips:\n",
        "    - RAG Mode: Best for straightforward questions with clear answers\n",
        "    - Agent Mode: Best for complex questions requiring deeper analysis\n",
        "    - Prompting Techniques:\n",
        "      - standard: Basic reasoning for straightforward questions\n",
        "      - cot (Chain of Thought): Step-by-step reasoning\n",
        "      - tot (Tree of Thought): Multiple reasoning branches\n",
        "      - got (Graph of Thought): Networked concept analysis\n",
        "    - Be specific in your questions\n",
        "    - Questions should be related to the content in the knowledge base or video\n",
        "    - You can optionally provide a YouTube video URL for additional context\n",
        "    - The system will provide relevant sources along with the answer\n",
        "    \"\"\")\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, inline=False, inbrowser=False)  # share=True creates a public URL"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7b-7ac6V3yi1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}